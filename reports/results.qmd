---
title: "Agentic AI Report"
format:
  html:
    toc: false
  pdf:
    toc: true
    pdf-engine: tectonic
  docx: default
  gfm: default
freeze: auto
execute:
  echo: false
---

```{r}
#| label: setup
library(yaml)
library(here)

fmt3 <- function(x) sprintf("%.3f", x)
fmt6 <- function(x) sprintf("%.6f", x)

cleaning <- yaml::read_yaml(here("outputs", "results", "cleaning.yml"))
base <- yaml::read_yaml(here("outputs", "results", "base_lm.yml"))
freq <- yaml::read_yaml(here("outputs", "results", "frequency_effect.yml"))
```

## Cleaning

The pipeline kept `r cleaning$counts$kept_trials` of `r cleaning$counts$total_trials` trials (dropped `r cleaning$counts$dropped_trials`).
Settings: correct-only = `r cleaning$trimming$correct_only`, RT range = `r cleaning$trimming$rt_min_ms`–`r cleaning$trimming$rt_max_ms` ms.

```{r}
#| label: cleaning-table
data.frame(
  setting = c(
    "correct_only",
    "rt_min_ms",
    "rt_max_ms",
    "total_trials",
    "kept_trials",
    "dropped_trials"
  ),
  value = c(
    as.character(cleaning$trimming$correct_only),
    cleaning$trimming$rt_min_ms,
    cleaning$trimming$rt_max_ms,
    cleaning$counts$total_trials,
    cleaning$counts$kept_trials,
    cleaning$counts$dropped_trials
  )
)
```

```{r}
#| label: rt-hist
knitr::include_graphics(here("outputs", "figures", "rt_hist.png"))
```

## Baseline model: frequency and strokes

```{r}
#| label: base-table
data.frame(
  term = c("intercept", "log_freq", "strokes"),
  estimate = c(
    fmt6(as.numeric(base$coefficients$intercept)),
    fmt6(as.numeric(base$coefficients$log_freq)),
    fmt6(as.numeric(base$coefficients$strokes))
  )
)
```

R² `r fmt3(as.numeric(base$r2))`; adjusted R² `r fmt3(as.numeric(base$adj_r2))`; residual sigma `r fmt3(as.numeric(base$sigma))`.
AIC `r fmt3(as.numeric(base$aic))`, BIC `r fmt3(as.numeric(base$bic))`.

## Frequency effect on recognition speed

```{r}
#| label: freq-table
freq_terms <- names(freq$coefficients)
freq_df <- data.frame(
  term = freq_terms,
  estimate = sapply(freq$coefficients, function(x) fmt6(as.numeric(x$estimate))),
  conf_low = sapply(freq$coefficients, function(x) fmt6(as.numeric(x$conf_low))),
  conf_high = sapply(freq$coefficients, function(x) fmt6(as.numeric(x$conf_high)))
)
freq_df

freq_effect <- freq$coefficients$log_freq
freq_multiplier <- exp(as.numeric(freq_effect$estimate))
freq_multiplier_low <- exp(as.numeric(freq_effect$conf_low))
freq_multiplier_high <- exp(as.numeric(freq_effect$conf_high))
freq_pct_drop <- (1 - freq_multiplier) * 100
freq_pct_drop_low <- (1 - freq_multiplier_high) * 100
freq_pct_drop_high <- (1 - freq_multiplier_low) * 100
pred_rt <- freq$reference$predicted_rt_ms
freq_quantiles <- freq$reference$log_freq_quantiles
```

A 1-unit increase in log<sub>10</sub> frequency (roughly a tenfold usage jump) is associated with an estimated
`r fmt3(freq_pct_drop)`% faster recognition (95% CI `r fmt3(freq_pct_drop_low)`% to `r fmt3(freq_pct_drop_high)`%) when strokes are held at their median
(`r freq$reference$strokes_median`). This corresponds to predicted mean response times of approximately
`r fmt3(unname(pred_rt["p010"]))` ms (10th percentile frequency),
`r fmt3(unname(pred_rt["p050"]))` ms (median), and
`r fmt3(unname(pred_rt["p090"]))` ms (90th percentile).
The marginal gains are largest through common characters—going from the median to the 90th percentile yields
`r fmt3(freq$flattening$delta_mid_high_ms)` ms faster responses—but they taper for the most extreme items:
the 10th to 50th percentile shift saves about `r fmt3(freq$flattening$delta_low_mid_ms)` ms, and the jump from the 90th to 95th percentile trims only
`r fmt3(freq$flattening$delta_high_top_ms)` ms.

```{r}
#| label: freq-plot
knitr::include_graphics(here(freq$figure))
```

A loess smooth over the character-level means reinforces the flattening:
after log<sub>10</sub> frequency around `r fmt3(unname(freq_quantiles["p090"]))`, the curve levels off with uncertainty bands that overlap tightly,
suggesting diminishing recognition-speed returns for the very most frequent characters.

<!--
To add another analysis:
1) create scripts/NN_slug.R that writes outputs/results/slug.yml
2) add a Makefile rule and add it to ANALYSES
3) copy this section, rename, and read slug.yml
Keep computation out of the QMD.
-->
